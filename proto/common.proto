syntax = "proto3";

option go_package = "github.com/vectorch-ai/scalellm;scalellm";
package llm.proto;

message Usage {
  // the number of tokens in the prompt.
  optional int32 prompt_tokens = 1 [json_name="prompt_tokens"];

  // the number of tokens in the generated completion.
  optional int32 completion_tokens = 2 [json_name="completion_tokens"];

  // the total number of tokens used in the request (prompt + completion).
  optional int32 total_tokens = 3 [json_name="total_tokens"];
}

enum Priority {
  DEFAULT = 0;

  HIGH = 1;

  NORMAL = 2;

  LOW = 3;
}

// Options for streaming response.
message StreamOptions{
  // if set, an additional chunk with usage will be streamed before the data: [DONE] message.
  optional bool include_usage = 1;
}
